---
title: "Another Lyme Disease CNN"
output: html_document
date: "2025-04-09"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This is the final version of the model, everything will work to run through it from top to bottom. Note that it will take about 2 hours to run the model on 20 epochs. 


Importing all the necessary libraries and files
```{r}
#install.packages("magick")
library(magick)
library(caret) 
set.seed(42)
```

Getting all of the negative images
```{r}
folder_path_negative <- "C:/Users/nmdej/OneDrive/DS4420/Negative"

image_files_negative <- list.files(folder_path_negative, pattern = "\\.jpg$", full.names = TRUE)

images_negative <- lapply(image_files_negative, image_read)

print(length(images_negative))
```

Getting all of the positive images
```{r}
folder_path_positive <- "C:/Users/nmdej/OneDrive/DS4420/Positive"

image_files_positive <- list.files(folder_path_positive, pattern = "\\.jpg$", full.names = TRUE)

images_positive <- lapply(image_files_positive, image_read)

print(length(images_positive))
```
Stacking the positive and negative images and creating the associated y column (1 = positive, 0 = negative)
```{r}
x <- c(images_positive, images_negative)
y <- c(rep(1, 941), rep(0, 4118))
```

Splitting into training and test sets
```{r}
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- x[train_indices]
X_test <- x[-train_indices]
y_train <- y[train_indices]
y_test <- y[-train_indices]
```

Building the model from scratch

Changing the matrix conversion function since padding is needed
```{r}
conv_to_mat2 <- function(x, im_shape = c(72, 72)) {
  x <- as.raster(x)
  x <- as.matrix(as.numeric(col2rgb(x)[1, ]))
  
  current_len <- length(x)
  target_size <- im_shape[1] * im_shape[2]
  
  # Adding some padding so all the images are the same size
  if(current_len < target_size) {
    x <- c(x, rep(0, target_size - current_len))
  } else if(current_len > target_size) {
    x <- x[1:target_size]
  }
  
  matrix(x, nrow = im_shape[1], ncol = im_shape[2], byrow = TRUE)
}
```

Creating the edge kernel and convolutional layer
```{r}
# edge kernel
W <- matrix(c(-1, 0, 1,
              -1, 0, 1,
              -1, 0, 1), nrow = 3, byrow = TRUE)

convMat <- function(X, W, stride=1) {
  # get the dimensions of the kernel and input
  k <- nrow(W)
  p <- nrow(X)
  
  # calculate the dimensions of the output matrix
  q <- (p - k) %/% stride + 1
  G <- matrix(0, nrow = q, ncol = q)
  
  # do the thing
  for (m in 1:q) {
    for (n in 1:q) {
      submatrix <- X[((m-1)*stride+1):((m-1)*stride+k), ((n-1)*stride+1):((n-1)*stride+k)]
      G[m, n] <- sum(W * submatrix)
    }
  }
  
  return(G)
}
```


Max Pooling Function
```{r}
max_pool <- function(G, pool_size) {
  m <- nrow(G)
  n <- ncol(G)
  
  # calculate dimensions of the pooled output
  pooled_height <- m - pool_size + 1
  pooled_width <- n - pool_size + 1
  pooled_output <- matrix(0, nrow = pooled_height, ncol = pooled_width)
  
  # do the thing
  for (i in 1:pooled_height) {
    for (j in 1:pooled_width) {
      # extract the pooling region
      pool_region <- G[i:(i + pool_size - 1), j:(j + pool_size - 1)]
      # take the maximum value
      pooled_output[i, j] <- max(pool_region)
    }
  }
  
  return(pooled_output)
}

```

Relu and Hidden Layer functions
```{r}
relu <- function(x){
  ifelse(x, x, 0)
}

hidden_layer <- function(x, w, activation = relu) {
  z <- x %*% w
  return(activation(z))
}
```

Binary loss and Sigmoid, as well as their derivative
```{r}
# Binary loss function
binary_loss <- function(y_true, y_pred) {
  eps <- 1e-7
  - (y_true * log(y_pred + eps) + (1 - y_true) * log(1 - y_pred + eps))
}

# Binary loss derivative 
binary_loss_deriv <- function(y_true, y_pred) {
  (y_pred - y_true) / (y_pred * (1 - y_pred) + 1e-7)
}

# Sigmoid funciotn
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}

# Sigmoid derivative
sigmoid_deriv <- function(x) {
  s <- sigmoid(x)
  s * (1 - s)
}
```

Leaky relu funcionts
```{r}
# leaky relu function
leaky_relu <- function(x, alpha=0.005) ifelse(x > 0, x, alpha*x)

# leaky relu derivative
leaky_relu_deriv <- function(x, alpha=0.005) ifelse(x > 0, 1, alpha)
```


Creating the CNN model from scratch
```{r}
# Defining our constants
learning_rate <- 0.001
epochs <- 20
batch_size <- 32 
lambda <- 0.001

# Utilizing the edge detection kernel
W <- matrix(c(-1, 0, 1,
              -1, 0, 1,
              -1, 0, 1), nrow = 3, byrow = TRUE)

# Getting set variables
sample_output <- convMat(conv_to_mat2(X_train[[1]]), W)
pooled_sample <- max_pool(sample_output, 2)
flat_sample <- as.vector(unlist(pooled_sample))
input_size <- length(flat_sample)
hidden_size <- 256

# Weight initialization with scaling
W1 <- matrix(rnorm(input_size * hidden_size, 0, sqrt(2/(input_size))), 
             nrow = input_size, ncol = hidden_size)
W2 <- matrix(rnorm(hidden_size, 0, sqrt(2/hidden_size)), 
             nrow = hidden_size, ncol = 1)
b1 <- matrix(0, nrow = 1, ncol = hidden_size)
b2 <- 0

# Momentum initialization
momentum_W1 <- matrix(0, nrow = input_size, ncol = hidden_size)
momentum_W2 <- matrix(0, nrow = hidden_size, ncol = 1)
momentum_b1 <- matrix(0, nrow = 1, ncol = hidden_size)
momentum_b2 <- 0

# Running our training model
for (epoch in 1:epochs) {
  total_loss <- 0
  correct <- 0
  total <- 0
  
  indices <- sample(1:length(X_train))
  
  # Running on batches, helped with convergence
  for (batch_start in seq(1, length(X_train), by = batch_size)) {
    batch_end <- min(batch_start + batch_size - 1, length(X_train))
    batch_indices <- indices[batch_start:batch_end]
    
    # Initialize batch gradients
    dW1_batch <- matrix(0, nrow = input_size, ncol = hidden_size)
    dW2_batch <- matrix(0, nrow = hidden_size, ncol = 1)
    db1_batch <- matrix(0, nrow = 1, ncol = hidden_size)
    db2_batch <- 0
    
    # Running in our batches
    for (i in batch_indices) {
      # Grabbing our images
      x_img <- conv_to_mat2(X_train[[i]])
      y_true <- y_train[i]
      
      # Forward pass
      conv_out <- convMat(x_img, W)
      pooled <- max_pool(conv_out, 2)
      flat_features <- as.vector(unlist(pooled))
      flat <- matrix(flat_features, nrow = 1)
      
      # Dense layer
      z1 <- flat %*% W1 + b1
      a1 <- leaky_relu(z1)
      
      # Output layer
      z2 <- a1 %*% W2 + b2
      y_pred <- sigmoid(z2)
      
      # Calculating loss
      loss <- binary_loss(y_true, y_pred)
      total_loss <- total_loss + loss
      
      # Backward pass
      dL_dy <- binary_loss_deriv(y_true, y_pred)
      dy_dz2 <- sigmoid_deriv(z2)
      dz2 <- dL_dy * dy_dz2
      dz1 <- (dz2 %*% t(W2)) * leaky_relu_deriv(z1)
      
      # Batch gradients updates
      dW1_batch <- dW1_batch + t(flat) %*% dz1
      dW2_batch <- dW2_batch + t(a1) %*% dz2
      db1_batch <- db1_batch + dz1
      db2_batch <- db2_batch + dz2
      
      # Accuracy
      predicted_class <- ifelse(y_pred > 0.5, 1, 0)
      correct <- correct + (predicted_class == y_true)
      total <- total + 1
    }
    
    # Average gradients over batch
    batch_size_actual <- length(batch_indices)
    dW1_batch <- dW1_batch / batch_size_actual + lambda * W1  # L2 regularization
    dW2_batch <- dW2_batch / batch_size_actual + lambda * W2
    db1_batch <- db1_batch / batch_size_actual
    db2_batch <- db2_batch / batch_size_actual
    
    # Update with momentum (proving us with information on PAST weights as well)
    momentum_W1 <- .9 * momentum_W1 + 0.1 * dW1_batch
    momentum_W2 <- .9 * momentum_W2 + 0.1 * dW2_batch
    momentum_b1 <- .9 * momentum_b1 + 0.1 * db1_batch
    momentum_b2 <- .9 * momentum_b2 + 0.1 * db2_batch
    
    # Weight update with momentum to help with convergence
    W1 <- W1 - learning_rate * momentum_W1
    W2 <- W2 - learning_rate * momentum_W2
    b1 <- b1 - learning_rate * momentum_b1
    b2 <- b2 - learning_rate * momentum_b2
  }
  
  # Calculate metrics
  avg_loss <- total_loss / length(X_train)
  accuracy <- correct / total
  
  # Printing out each epoch
  cat(sprintf("Epoch %d - Loss: %.4f - Accuracy: %.2f%%\n", 
              epoch, avg_loss, accuracy*100))
}

```

Testing the model performance
```{r}
test_predictions <- vector("numeric", length = length(X_test))
test_probabilities <- vector("numeric", length = length(X_test))
correct <- 0
total <- 0

# Testing the accuracy of the model
for (i in 1:length(X_test)) {
    x_img <- conv_to_mat2(X_test[[i]])
    y_true <- y_test[i]
    
    # Forward pass
    conv_out <- convMat(x_img, W)
    pooled <- max_pool(conv_out, 2)
    flat_features <- as.vector(unlist(pooled))
    flat <- matrix(flat_features, nrow = 1)
    z1 <- flat %*% W1 + b1  # Trained W2
    a1 <- leaky_relu(z1)
    
    # Out layer
    z2 <- a1 %*% W2 + b2    # Trained W2
    # Sigmoid since it is binary classification
    y_pred_prob <- sigmoid(z2)
    
    # Store predictions and probabilities
    test_predictions[i] <- ifelse(y_pred_prob > 0.5, 1, 0)
    test_probabilities[i] <- y_pred_prob
    
    # Accuracy
    correct <- correct + (test_predictions[i] == y_true)
    total <- total + 1
}

# Calculate final test accuracy
test_accuracy <- correct / total
cat(sprintf("Test Accuracy: %.2f%%\n", test_accuracy * 100))

```
Creating a Confusion Matrix to analyze results
```{r}
# Modeling our results in a confusion matrix
conf_mat <- confusionMatrix(factor(test_predictions), factor(y_test))
fourfoldplot(conf_mat$table, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")
```
