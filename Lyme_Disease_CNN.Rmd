---
title: "Lyme Disease CNN"
output: html_document
date: "2025-03-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Importing all the necessary libraries and files
```{r}
install.packages("magick")
library(magick)
library(caret) 
```

Getting all of the negative images
```{r}
folder_path_negative <- "C:/Users/nmdej/OneDrive/DS4420/Negative"

image_files_negative <- list.files(folder_path_negative, pattern = "\\.jpg$", full.names = TRUE)

images_negative <- lapply(image_files_negative, image_read)

print(length(images_negative))
```

Getting all of the positive images
```{r}
folder_path_positive <- "C:/Users/nmdej/OneDrive/DS4420/Positive"

image_files_positive <- list.files(folder_path_positive, pattern = "\\.jpg$", full.names = TRUE)

images_positive <- lapply(image_files_positive, image_read)

print(length(images_positive))
```
Stacking the positive and negative images and creating the associated y column (1 = positive, 0 = negative)
```{r}
x <- c(images_positive, images_negative)
y <- c(rep(1, 941), rep(0, 4118))
```

Splitting into training and test sets
```{r}
train_indices <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- x[train_indices]
X_test <- x[-train_indices]
y_train <- y[train_indices]
y_test <- y[-train_indices]
```

Building the model from scratch
```{r}
conv_to_mat <- function(x, im_shape = c(50, 50)) {
  # need to convery magick to a matrix
  x <- as.raster(x)
  x <- as.matrix(as.numeric(col2rgb(x)[1, ]))

  x <- matrix(x, nrow = im_shape[1], ncol = im_shape[2], byrow = TRUE)
}

plot_img <- function(x, im_shape = c(50, 50)) {
  
  # need to convert matrix to data frame for ggplot
  df <- data.frame(expand.grid(x = 1:im_shape[2], y = 1:im_shape[1]),
                   value = as.vector(t(x)))
  
  ggplot(df, aes(x, y, fill = value)) +
    geom_tile() +
    # can choose how to map the 0 and 1 (I'm copying cmap from python here)
    scale_fill_gradient(low = "black", high = "white") +
    theme_void() +
    theme(aspect.ratio = 1) +
    scale_y_reverse()
}

plot_img(conv_to_mat(X_train[[1]]))
```
```{r}
conv_to_mat2 <- function(x, im_shape = c(50, 50)) {
  # Convert magick image to raster then to numeric matrix
  x <- as.raster(x)
  x <- as.matrix(as.numeric(col2rgb(x)[1, ]))
  
  # Calculate current dimensions
  current_len <- length(x)
  target_size <- im_shape[1] * im_shape[2]
  
  # Add padding if needed
  if(current_len < target_size) {
    # Pad with zeros (or NA/median value)
    x <- c(x, rep(0, target_size - current_len))
  } else if(current_len > target_size) {
    # Truncate if too large
    x <- x[1:target_size]
  }
  
  # Reshape to target dimensions
  matrix(x, nrow = im_shape[1], ncol = im_shape[2], byrow = TRUE)
}
```

```{r}
# edge kernel
W <- matrix(c(-1, 0, 1,
              -1, 0, 1,
              -1, 0, 1), nrow = 3, byrow = TRUE)

convMat <- function(X, W, stride=1) {
  # get the dimensions of the kernel and input
  k <- nrow(W)
  p <- nrow(X)
  
  # calculate the dimensions of the output matrix
  q <- (p - k) %/% stride + 1
  G <- matrix(0, nrow = q, ncol = q)
  
  # do the thing
  for (m in 1:q) {
    for (n in 1:q) {
      submatrix <- X[((m-1)*stride+1):((m-1)*stride+k), ((n-1)*stride+1):((n-1)*stride+k)]
      G[m, n] <- sum(W * submatrix)
    }
  }
  
  return(G)
}
```

```{r}
X <- conv_to_mat2(X_train[[1]])
G <- convMat(X, W, stride=2)
print(G)

plot_img(G, im_shape = c(24, 24))
```
Max Pooling Function
```{r}
max_pool <- function(G, pool_size) {
  m <- nrow(G)
  n <- ncol(G)
  
  # calculate dimensions of the pooled output
  pooled_height <- m - pool_size + 1
  pooled_width <- n - pool_size + 1
  pooled_output <- matrix(0, nrow = pooled_height, ncol = pooled_width)
  
  # do the thing
  for (i in 1:pooled_height) {
    for (j in 1:pooled_width) {
      # extract the pooling region
      pool_region <- G[i:(i + pool_size - 1), j:(j + pool_size - 1)]
      # take the maximum value
      pooled_output[i, j] <- max(pool_region)
    }
  }
  
  return(pooled_output)
}

```

```{r}
PG <- max_pool(G, 2)
PG

plot_img(PG, im_shape = c(23, 23))
```

```{r}
padding <- function(X, W) {
  # get the dimensions of the kernel and input
  k <- nrow(W)
  p <- nrow(X)
  
  px <- (k - 1) %/% 2
  
  # add the top and bottom pads
  pad1 <- matrix(0, nrow = px, ncol = p)
  padX1 <- rbind(pad1, X, pad1)
  
  # add the left and right pads
  pad2 <- matrix(0, nrow = nrow(padX1), ncol = px)
  padX2 <- cbind(pad2, padX1, pad2)
  
  return(padX2)
}
```

Relu, Flatten, and Hidden Layer functions
```{r}
relu <- function(x){
  ifelse(x, x, 0)
}

flatten <- function(x) {
  return(as.vector(t(x)))
}

hidden_layer <- function(x, w, activation = relu) {
  z <- x %*% w
  return(activation(z))
}
```

Just a singular forward function
```{r}
cnn_forward <- function(image, weights) {
  convMat1 <- convMat(image, weights)
  
  pooled_outputs <- max_pool(convMat1, 2)
  
  flat_features <- unlist(lapply(pooled_outputs, flatten))
  
  input_size <- length(flat_features)
  print(input_size)
  
  W1 <- matrix(rnorm(input_size * 16, mean = 0, sd = 0.1), nrow = input_size, ncol = 16)
  
  hid_layer <- hidden_layer(flat_features, W1)
  
  W2 <- matrix(rnorm(1, mean = 0, sd = 0.1), nrow = 16, ncol = 1)
  
  out_layer <- hidden_layer(hid_layer, W2)
  
  sigmoid <- function(x) 1 / (1 + exp(-x))
  final_output <- sigmoid(out_layer)
  
  return(final_output)
}

```

```{r}
lets_see <- cnn_forward(X, W)

print(lets_see)
```

```{r}
binary_loss <- function(y_true, y_pred) {
  eps <- 1e-7
  - (y_true * log(y_pred + eps) + (1 - y_true) * log(1 - y_pred + eps))
}

binary_loss_deriv <- function(y_true, y_pred) {
  (y_pred - y_true) / (y_pred * (1 - y_pred) + 1e-7)
}

sigmoid <- function(x) {
  1 / (1 + exp(-x))
}

sigmoid_deriv <- function(x) 
  s <- sigmoid(x)
  s * (1 - s)
}

relu_deriv <- function(x) as.numeric(x > 0)
```


NEWEST ONE
```{r}
learning_rate <- 0.001
epochs <- 5

# Initialize weights and biases dynamically based on CNN output
# First get output size from a sample
sample_output <- convMat(conv_to_mat2(X_train[[1]]), W)
pooled_sample <- max_pool(sample_output, 3)
flat_sample <- as.vector(unlist(pooled_sample))

input_size <- length(flat_sample)  # Set dynamically based on CNN output
hidden_size <- 64

# Initialize weights and biases
W1 <- matrix(rnorm(input_size * hidden_size, 0, sqrt(2/input_size)), 
             nrow = input_size, ncol = hidden_size)

W2 <- matrix(rnorm(hidden_size, 0, sqrt(2/hidden_size)), 
             nrow = hidden_size, ncol = 1)

b1 <- matrix(0, nrow = 1, ncol = hidden_size)
b2 <- 0

for (epoch in 1:epochs) {
  total_loss <- 0
  correct <- 0
  total <- 0
  
  for (i in 1:length(X_train)) {
    # --- Forward ---
    x_img <- conv_to_mat2(X_train[[i]])
    y_true <- y_train[i]
    
    # CNN forward pass
    conv_out <- convMat(x_img, W)
    pooled <- max_pool(conv_out, 3)
    flat_features <- as.vector(unlist(pooled))
    
    # Ensure flat_features is a column vector
    flat <- matrix(flat_features, nrow = 1)  # Convert to 1 x input_size matrix
    
    # Hidden layer forward pass
    z1 <- flat %*% W1
    a1 <- leaky_relu(z1)
    
    # Output layer forward pass
    z2 <- a1 %*% W2
    y_pred <- sigmoid(z2)
    
    # --- Loss ---
    loss <- binary_loss(y_true, y_pred)
    total_loss <- total_loss + loss
    
    # --- Backpropagation ---
    dL_dy <- binary_loss_deriv(y_true, y_pred)
    dy_dz2 <- sigmoid_deriv(z2)
    dz2 <- dL_dy * dy_dz2
    
    # Gradients for W2 and b2
    dW2 <- t(a1) %*% dz2
    db2 <- dz2

    # Backprop to hidden
    dz1 <- (dz2 %*% t(W2)) * leaky_relu_deriv(z1)
    
    # Gradients for W1 and b1
    dW1 <- t(flat) %*% dz1
    db1 <- dz1
    
    # --- Update weights and biases ---
    W2 <- W2 - learning_rate * dW2
    b2 <- b2 - learning_rate * db2

    W1 <- W1 - learning_rate * dW1
    b1 <- b1 - learning_rate * db1
    
    # accuracy
    predicted_class <- ifelse(y_pred > 0.5, 1, 0)
    correct <- correct + (predicted_class == y_true)
    total <- total + 1
  }
  
  avg_loss <- total_loss / length(X_train)
  accuracy <- correct / total
  
  cat(sprintf("Epoch %d - Loss: %.4f - Accuracy: %.2f%%\n", 
              epoch, avg_loss, accuracy*100))

  
}
```


```{r}
leaky_relu <- function(x, alpha=0.01) ifelse(x > 0, x, alpha*x)
leaky_relu_deriv <- function(x, alpha=0.01) ifelse(x > 0, 1, alpha)
```
